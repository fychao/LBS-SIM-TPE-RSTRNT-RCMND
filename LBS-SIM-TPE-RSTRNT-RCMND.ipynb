{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Start pylab inline mode, so figures will appear in the notebook\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# include libs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zpickle import *\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "import operator\n",
    "import random\n",
    "import chilin\n",
    "from sinica import getPOS\n",
    "from joblib import Parallel, delayed\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "from itertools import cycle\n",
    "import pylab as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fn = \"./zobj/ALL_DATA_183340.zobj\"\n",
    "# oData = load(fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "attrs = load(\"./zobj/shops_attr_183340.zobj\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "stat = {}\n",
    "for ele in oData:\n",
    "    if not stat.has_key(ele['domain']):\n",
    "        stat[ ele['domain'] ] = {}\n",
    "    if not stat[ ele['domain'] ].has_key(ele['shop_id']):\n",
    "        stat[ ele['domain'] ][ ele['shop_id'] ] = []\n",
    "    if attrs.has_key( ele['shop_id'] ) and attrs[ ele['shop_id'] ].has_key('addr'):\n",
    "        addr = attrs[ ele['shop_id'] ]['addr']\n",
    "        if re.match(u\"^台北市\", addr):\n",
    "            stat[ ele['domain'] ][ ele['shop_id'] ].append(ele)\n",
    "    else:\n",
    "        print ele['shop_id']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "wanted_opn = []\n",
    "shops = []\n",
    "for domain in stat.keys():\n",
    "    for shop in stat[domain]:\n",
    "        pos_num = 0\n",
    "        neg_num = 0\n",
    "        tmp_opn = []\n",
    "        for opn in stat[domain][shop]:\n",
    "            if int(opn['stat_mvalue']) > 50:\n",
    "                pos_num += 1\n",
    "            elif int(opn['stat_mvalue']) <= 40:\n",
    "                neg_num += 1\n",
    "            tmp_opn.append(opn)\n",
    "        if pos_num > 5 and neg_num > 5:\n",
    "            shops.append(shop)\n",
    "            wanted_opn.extend( tmp_opn )\n",
    "            \n",
    "print len(wanted_opn)\n",
    "print \"shops:\", len(shops)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print attrs[wanted_opn[0]['shop_id']]['addr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看餐廳的地理參數"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "uniq_shop = list(set([ x['shop_id'] for x in wanted_opn]))\n",
    "print len(uniq_shop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class zTags():\n",
    "    ''' 所有的 POS TAG 元素 '''\n",
    "    @staticmethod\n",
    "    def getEol():\n",
    "        EOLS = [\n",
    "            \"FW\",\n",
    "            \"QUESTIONCATEGORY\",\n",
    "            \"COLONCATEGORY\",\n",
    "            \"COMMACATEGORY\",\n",
    "            \"DASHCATEGORY\",\n",
    "            \"ETCCATEGORY\",\n",
    "            \"PARENTHESISCATEGORY\",\n",
    "            \"PAUSECATEGORY\",\n",
    "            \"PERIODCATEGORY\",\n",
    "            \"QUESTIONCATEGORY\",\n",
    "            \"SEMICOLONCATEGORY\",\n",
    "            \"EXCLANATIONCATEGORY\",\n",
    "            \"EXCLAMATIONCATEGORY\",\n",
    "            \"BR\",\n",
    "            \"SPCHANGECATEGORY\"]\n",
    "        return set(EOLS)\n",
    "    \n",
    "    @staticmethod\n",
    "    def isEol(tag):\n",
    "        EOLS = zTags.getEol()\n",
    "        return True if tag in EOLS else False\n",
    "        \n",
    "    ''' 過濾出不要的標記 '''\n",
    "    @staticmethod\n",
    "    def isWant(tag):\n",
    "        NOT_WANT = [\"^P\", \"^C\", \"^D\", \"^N[c|d|e|f|g|h]\", \"V_2\", \"^T\", \"^SHI\"]\n",
    "        NOT_WANT.extend(zTags.getEol())\n",
    "        flg = True\n",
    "        NOT_WANT = set(NOT_WANT)\n",
    "        for ele in NOT_WANT:\n",
    "            if re.match(ele, tag):\n",
    "                flg = False\n",
    "        return flg\n",
    "                \n",
    "    @staticmethod\n",
    "    def isNot(word):\n",
    "        ''' 否定句 '''\n",
    "        # \"但是\", \"但\", \n",
    "        NOTS = [u\"不\", u\"沒有\", u\"不要\", u\"不能\", u\"沒\", u\"沒有\", u\"無\", u\"不會\", u\"難\", u\"算不上\", u\"未\"]\n",
    "        NOTS = set(NOTS)\n",
    "        return True if word in NOTS else False\n",
    "        \n",
    "    @staticmethod\n",
    "    def notMorph(word):\n",
    "        '''帶否定語素'''\n",
    "        NOTs = u\"^[不|難|沒|未]\"\n",
    "        return re.sub(NOTs, \"\", word) if re.match(NOTs, word) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class zWord():\n",
    "    ''' 基本單詞元素 '''\n",
    "    def __init__(self, word, go_prev=None, go_next=None):\n",
    "        self.org = word\n",
    "        (word, tag) = zWord.toElements(word)\n",
    "        self.word = word\n",
    "        \n",
    "        self.tag = tag\n",
    "        \n",
    "        ''' 設定 wanted '''\n",
    "        self.wanted = zTags.isWant(tag)\n",
    "        \n",
    "        ''' 設定 ORI '''\n",
    "        if zTags.isNot(self.word):\n",
    "            self.ori = 0\n",
    "            self.wanted = False\n",
    "        else:\n",
    "            self.ori = 1\n",
    "\n",
    "        # 詞中有否定\n",
    "        self._selfNot()\n",
    "            \n",
    "        # 取得最佳表達式\n",
    "        self.word_best = self.get_best()\n",
    "\n",
    "        \n",
    "    def get_best(self):\n",
    "        \"\"\"\n",
    "            取得最好的表達式， 例如： \"不\" -> \"\"； \"不用\" -> \"用-\"\n",
    "        \"\"\"\n",
    "        if not self.wanted:\n",
    "            # 詞類不要\n",
    "            return \"\"\n",
    "        if self.ori == 0:\n",
    "            # 否定字組\n",
    "            return \"\"\n",
    "        \n",
    "        return self.word.replace(\"-\", \"\") if self.ori > 0 else self.word.replace(\"-\", \"\")+u\"-\" \n",
    "    \n",
    "    def _selfNot(self):\n",
    "        '''詞中帶有否定'''\n",
    "        if not zTags.isNot(self.word) and zTags.notMorph(self.word): # 要改這裡\n",
    "            if not len(self.word) == 1: # 要改這裡\n",
    "                '''多語素'''\n",
    "                self.ori = 0\n",
    "                # 帶\"不\"頭\n",
    "                self.word = zTags.notMorph(self.word)\n",
    "                self.wanted = True\n",
    "            else:\n",
    "                '''單語素'''\n",
    "                self.ori = 0\n",
    "                self.word = \"\"\n",
    "                self.wanted = False\n",
    "        \n",
    "    ''' Toggle Negative '''\n",
    "    def toggle(self):\n",
    "        if self.ori == 1:\n",
    "            self.ori =-1 \n",
    "        elif self.ori ==-1:\n",
    "            self.ori = 1 \n",
    "        \n",
    "        \n",
    "    ''' 分開 tag 與 word '''\n",
    "    @staticmethod\n",
    "    def toElements(word):\n",
    "        eles = word.replace(u\")\", u\"\").split(u\"(\")\n",
    "        if len(eles)==2:\n",
    "            return (eles[0], eles[1])\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"[%s] %s(%s:%s:%s)\"%(self.__class__.__name__, \n",
    "                              self.word_best.encode(\"utf8\"),\n",
    "                              self.tag.encode(\"utf8\"),\n",
    "                              self.ori,\n",
    "                              self.wanted\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class zSentence:\n",
    "    ''' NOTE: 引入的 stn 一定要先用 toWords 切開\n",
    "        單句元素，會包 zWord \n",
    "    '''\n",
    "    def __init__(self, stn):\n",
    "        if type(stn)==type([]):\n",
    "            self.stn_org = stn\n",
    "        if type(stn)==type(u\"\"):\n",
    "            self.stn_org = zSentence.toWords(stn)\n",
    "        self.proc()\n",
    "\n",
    "    \"\"\" 處理評論句子內容 \"\"\"\n",
    "    def proc(self):\n",
    "        words = []\n",
    "        \n",
    "        for word in self.stn_org:\n",
    "            words.append( zWord(word) )\n",
    "            \n",
    "        self.words = words\n",
    "        \n",
    "        self.notOperation()\n",
    "        \n",
    "#         self._define_bigrams()\n",
    "\n",
    "#     def get_bigrams(self, with_best=False, is_radicals=False):\n",
    "#         if with_best :\n",
    "#             if is_radicals:\n",
    "#                 return self.bigrams_best_radical\n",
    "#             else:\n",
    "#                 return self.bigrams_best\n",
    "#         else:\n",
    "#             if is_radicals:\n",
    "#                 return self.bigrams_radical\n",
    "#             else:\n",
    "#                 return self.bigrams \n",
    "        \n",
    "#     def _define_bigrams(self, n=5, with_best=False):\n",
    "#         skipWindow = zSentence.skipWindow\n",
    "#         self.bigrams_best = set([(tuples[0].get_best(), tuples[1].get_best()) for tuples in skipWindow(self.words, n) if tuples[0].wanted and tuples[1].wanted and not tuples[0].word == tuples[1].word])\n",
    "#         self.bigrams = set([(tuples[0].word, tuples[1].word) for tuples in skipWindow(self.words, n) if tuples[0].wanted and tuples[1].wanted and not tuples[0].word == tuples[1].word])\n",
    "        \n",
    "#         self.bigrams_best_radical = set([ (term2Radi(x), term2Radi(y)) for (x,y) in self.bigrams_best ])\n",
    "#         self.bigrams_radical = set([ (term2Radi(x), term2Radi(y)) for (x,y) in self.bigrams ])\n",
    "        \n",
    "    def get_words(self):\n",
    "        return [ wrd for wrd in self.words] \n",
    "    \n",
    "    def get_wanted(self):\n",
    "        ''' 取得需要的字元 '''\n",
    "        return [ wrd for wrd in self.words if wrd.wanted]\n",
    "        \n",
    "    def notOperation(self):\n",
    "        \"\"\" 否定句處理 \"\"\"\n",
    "        for idx in range(len(self.words)):\n",
    "            if self.words[idx].ori == 0:\n",
    "                for idy in range(idx, len(self.words)):\n",
    "                    self.words[idy].toggle()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"[%s] Words:%s\"%(self.__class__.__name__, len(self.words)) \n",
    "        \n",
    "    @staticmethod\n",
    "    def toWords(stn):\n",
    "        ''' 開成字段 '''\n",
    "        if re.search(u\"　\", stn):\n",
    "            return stn.split(u\"　\")\n",
    "        if re.search(u\" \", stn):\n",
    "            return stn.split(u\" \")\n",
    "      \n",
    "    @staticmethod\n",
    "    def skipWindow(seq, max_win=5):\n",
    "        \"\"\"\n",
    "            SKIP Window 算法\n",
    "        \"\"\"\n",
    "        olen = len(seq)\n",
    "        rterms = []\n",
    "        for pivot in range(olen):\n",
    "            left  = (pivot - max_win) if (pivot - max_win) > 0 else 0\n",
    "            right = (pivot + max_win) if (pivot + max_win) < olen else olen\n",
    "\n",
    "            for idx in range(left, right):\n",
    "                if not idx == pivot and (seq[pivot].wanted and seq[idx].wanted):\n",
    "                    \"\"\" 回傳組合，不從順序，從筆劃 \"\"\"\n",
    "                    if len(seq[pivot].word[0]) > 0 and len(seq[idx].word[0]) > 0:\n",
    "                        if seq[pivot].word[0] > seq[idx].word[0]:\n",
    "                            rterms.append( ( seq[pivot], seq[idx] ) ) \n",
    "                        else:\n",
    "                            rterms.append( ( seq[idx], seq[pivot] ) ) \n",
    "\n",
    "        return set(rterms)\n",
    " \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class zOpinion:\n",
    "    \"\"\" 單一評論 \"\"\"\n",
    "    def __init__(self, opn):\n",
    "        self.data = opn\n",
    "        self.stns = []\n",
    "        self.word_only = []\n",
    "        self.proc()\n",
    "        \n",
    "    \"\"\" 處理評論內容 \"\"\"\n",
    "    def proc(self):\n",
    "        stns = zOpinion.toStn(self.data)\n",
    "        for stn in stns:\n",
    "            self.stns.append(zSentence(stn))\n",
    "            \n",
    "        self.stat()\n",
    "        self._doWords4Tag()\n",
    "        \n",
    "    \"\"\" 評論內的字數統計 \"\"\"\n",
    "    def stat(self):\n",
    "        \n",
    "        ''' 單一詞統計 '''\n",
    "        oWords = []\n",
    "        \n",
    "        ''' bigram 統計 '''\n",
    "        oBigrams = []\n",
    "        oRadiBigrams = []\n",
    "        for stn in self.stns:\n",
    "            self.word_only.extend([x.word for x in stn.get_words()])\n",
    "            oWords.extend([ x.word_best for x in stn.get_wanted()])\n",
    "#             oBigrams.extend(stn.get_bigrams())\n",
    "#             oRadiBigrams.extend([ (term2Radi(x), term2Radi(y)) for (x, y) in stn.get_bigrams()])\n",
    "            \n",
    "        # 單一詞結果\n",
    "        self.stat_words = oWords\n",
    "        self.stat_words_dic = collections.Counter(oWords)\n",
    "        \n",
    "        # 雙詞的結果\n",
    "#         self.stat_bigrams = oBigrams\n",
    "#         self.stat_bigrams_dic = collections.Counter(oBigrams)\n",
    "        \n",
    "        # 雙詞的部首\n",
    "#         self.stat_radi_bigram = oRadiBigrams\n",
    "        \n",
    "    def getOrg(self):\n",
    "        return self.data['tagged']\n",
    "        \n",
    "    \"\"\" 取得特定 key 的值 \"\"\"\n",
    "    def get(self, key):\n",
    "        if key in self.getKeys():\n",
    "            return self.data[key]\n",
    "        \n",
    "    def _doWords4Tag(self):\n",
    "        wrds = []\n",
    "        for stn in self.stns:\n",
    "            for word in stn.get_wanted():\n",
    "                wrds.append(word.get_best()) # here\n",
    "        self._stn_words = wrds\n",
    "#         self._stn_radical_pairs = [ term2Radi(x) for x in wrds]\n",
    "\n",
    "    def getWords4Tag(self):\n",
    "        return self._stn_words\n",
    "    \n",
    "    def getRadicalsPair4Tag(self):\n",
    "        return self._stn_radical_pairs\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"[%s] words:%s\"%(self.__class__.__name__, len(self.stat_words_dic.keys()))  \n",
    "\n",
    "        \n",
    "    def getPair(self, wanted_terms):\n",
    "        \"\"\" 取得必要的 pair \"\"\"\n",
    "\n",
    "#         sorted2Terms = lambda x, y: (x, y) if x > y else (y, x)\n",
    "#         isInBigram = lambda x,y: sorted2Terms(x,y) if sorted2Terms(x,y) in self.stat_bigrams_dic.keys() else False\n",
    "\n",
    "        wanted_pairs = [ x for x in self.stat_bigrams_dic.keys() if x[0] in wanted_terms or x[1] in wanted_terms]\n",
    "\n",
    "        return_pairs = []\n",
    "        for stn in self.stns:\n",
    "            stn_terms = list(set([ wrd.word_best for wrd in stn.get_wanted()]))\n",
    "            for pair in wanted_pairs:\n",
    "                size = len(set([ x.replace(\"-\", \"\") for x in stn_terms]).intersection(set(pair)))\n",
    "                if size >= 1:\n",
    "                    org_x = \"\".join([ x for x in stn_terms if x.replace(\"-\", \"\") == pair[0]])\n",
    "                    org_y = \"\".join([ x for x in stn_terms if x.replace(\"-\", \"\") == pair[1]])\n",
    "                    return_pairs.append( \"%s/%s\"%(org_x, org_y) )\n",
    "            \n",
    "        return list(set(return_pairs))\n",
    "    \n",
    "    @staticmethod\n",
    "    def toStn(stn):\n",
    "        ''' 開成字段, [ [word, word], [] ... ] '''\n",
    "        tagged = u\"　 \".join(stn.split(\"\\n\"))\n",
    "        words = zSentence.toWords(tagged)\n",
    "        stns = []\n",
    "        stn = []\n",
    "        for idx in range(len(words)):\n",
    "            ''' 可能會有空字串 '''\n",
    "            try:\n",
    "                (word, tag) = zWord.toElements(words[idx])\n",
    "                stn.append(words[idx])\n",
    "                if zTags.isEol(tag):\n",
    "                    # 如果字串少於1 個字，就跳開\n",
    "                    if len(stn) >1:\n",
    "                        stns.append(stn)\n",
    "                    stn = []\n",
    "            except: \n",
    "                continue\n",
    "        if len(stn) >1:\n",
    "            stns.append(stn)\n",
    "\n",
    "        return stns\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "oCol = []\n",
    "idx = 0\n",
    "rank = []\n",
    "time_start = time.clock()\n",
    "\n",
    "for ele in wanted_opn[:]:\n",
    "    rank.append( ele['stat_mvalue'] )\n",
    "    if (idx%100) == 0:\n",
    "        print idx, \" time: %.2gs\" % (time.clock()-time_start)\n",
    "        time_start = time.clock()\n",
    "\n",
    "    idx += 1\n",
    "    oCol.append(( attrs[ele['shop_id']], \n",
    "                  getChilinTxt(ele['tagged']),\n",
    "                  ele['stat_mvalue'],\n",
    "                  ele['tagged']\n",
    "                ))\n",
    "\n",
    "print collections.Counter(rank)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print len(oCol)\n",
    "save(oCol, \"./zobj/Taipei_shops_comments.zobj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 由此開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oChi = chilin.chilin()\n",
    "\n",
    "def cnv2chilin(wrd):\n",
    "    ''' 使用詞林擴充 '''\n",
    "    tobj = oChi.getHead(wrd.encode(\"utf8\"), 8)\n",
    "    if tobj:\n",
    "        tobj = [ x for x in tobj if (not x[0] in ['A', 'C', 'J', 'K', 'L']) and not x[-1] in ['#']]\n",
    "        if len(tobj)>0:\n",
    "#             print wrd, \" \".join(tobj)\n",
    "            grps = [ [ y.decode(\"utf8\") for y in oChi.getWrd(x)] for x in tobj ] \n",
    "            return set([item for sublist in grps for item in sublist]) # see http://bit.ly/1FrTjwF\n",
    "        return set([wrd])\n",
    "    else:\n",
    "        return set([wrd])\n",
    "    \n",
    "def chilinExt(aSet):\n",
    "    # 使用辭林擴充\n",
    "    ext = set()\n",
    "\n",
    "    if type(aSet) == type(set()):\n",
    "        for x in aSet:\n",
    "            flg = False\n",
    "            if re.search(\"-$\", x):\n",
    "                flg = True\n",
    "\n",
    "            cnvted = cnv2chilin(x.replace(\"-\", \"\"))\n",
    "            ext.update([\"%s\"%x if not flg else \"%s-\"%x for x in cnvted])\n",
    "    elif type(aSet) == type(u\"\"):\n",
    "        flg = False\n",
    "        if re.search(\"-$\", aSet):\n",
    "            flg = True\n",
    "        cnvted = cnv2chilin(aSet.replace(\"-\", \"\"))\n",
    "        ext.update([\"%s\"%x if not flg else \"%s-\"%x for x in cnvted])\n",
    "        \n",
    "    return \" \".join(ext).replace(\"-\", u\"負\")\n",
    "\n",
    "def getChilinTxt(tagged):\n",
    "    opn = zOpinion(tagged)\n",
    "#     print len(opn.stns)\n",
    "    txts = Parallel(n_jobs=-1)(delayed(chilinExt)(set([x.get_best() for x in stn.get_wanted()])) for stn in opn.stns)\n",
    "#     for stn in opn.stns:\n",
    "#         print \" \".join(stn.stn_org)\n",
    "# #         print \" \".join([\"%s, %s\\n\"%(x.get_best(), \" \".join(chilinExt(x.get_best()))) for x in stn.get_wanted()])\n",
    "#         print chilinExt(set([x.get_best() for x in stn.get_wanted()]))\n",
    "        \n",
    "    return \" \".join(txts)\n",
    "# print \" \".join(chilinExt([u\"錯\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "oCol = load(\"./zobj/Taipei_shops_comments.zobj\")\n",
    "print len(oCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//1 build dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "txts = [ x[1].split(\" \") for x in oCol]\n",
    "dictionary = corpora.Dictionary(txts)\n",
    "dictionary.filter_tokens(bad_ids=[0])\n",
    "dictionary.filter_extremes(no_below=5)\n",
    "dictionary.compactify()\n",
    "len(dictionary.token2id)\n",
    "\n",
    "dictionary.save(\"./zobj/16908_dictionary.mm\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ntusd_hash = load(\"./zobj/NTUSD_HASH_POS_NEG.zobj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65389\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary.load(\"./zobj/16908_dictionary.mm\")\n",
    "print len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank_code = lambda x: \"P\" if int(x)>50 else \"N\"\n",
    "\n",
    "\n",
    "# print rank_code(oCol[1][2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "txts_pos = [ x for x in oCol if rank_code(x[2])==\"P\"]\n",
    "txts_neg = [ x for x in oCol if rank_code(x[2])==\"N\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save(txts_pos, \"./zobj/Taipei_shop_Txt_pos.zobj\")\n",
    "save(txts_neg, \"./zobj/Taipei_shop_Txt_neg.zobj\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txts_pos = load(\"./zobj/Taipei_shop_Txt_pos.zobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "corpus_pos = [ dictionary.doc2bow(x[1].split(\" \")) for x in txts_pos]\n",
    "corpus_neg = [ dictionary.doc2bow(x[1].split(\" \")) for x in txts_neg]\n",
    "print len(corpus_pos), len(corpus_neg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "corpora.MmCorpus.serialize('./zobj/Taipei_shop_corpus_pos.mm', corpus_pos)\n",
    "corpora.MmCorpus.serialize('./zobj/Taipei_shop_corpus_neg.mm', corpus_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_pos = corpora.MmCorpus('./zobj/Taipei_shop_corpus_pos.mm')\n",
    "corpus_neg = corpora.MmCorpus('./zobj/Taipei_shop_corpus_neg.mm')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tfidf_pos = models.TfidfModel(corpus_pos)\n",
    "lsi = models.LsiModel(tfidf_pos[corpus_pos], num_topics=200, id2word=dictionary)\n",
    "lsi.save('./zobj/Taipei_shops_LSI.index')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "p = re.compile(ur'(　\\w+\\([a-zA-Z0-9]*\\))', re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel.load('./zobj/Taipei_shops_LSI.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035*\"蛋糕\" + 0.022*\"燔\" + 0.022*\"麵包\" + 0.022*\"拉麵\" + 0.022*\"鬆餅\" + 0.021*\"巧克力\" + 0.021*\"炕\" + 0.021*\"炙\" + 0.021*\"烤\" + 0.021*\"烘製\"\n",
      "0.049*\"蛋糕\" + -0.041*\"可觀負\" + -0.041*\"口碑載道負\" + -0.041*\"交口稱譽負\" + -0.041*\"有口皆碑負\" + -0.041*\"名不虛傳負\" + -0.041*\"甚佳負\" + -0.041*\"拔尖負\" + -0.041*\"優負\" + -0.041*\"佳負\"\n",
      "-0.080*\"辣味\" + -0.080*\"麻辣\" + -0.080*\"辛\" + -0.080*\"辣絲絲\" + -0.080*\"辣乎乎\" + -0.080*\"辛辣\" + 0.077*\"蛋糕\" + -0.072*\"拉麵\" + -0.065*\"辣\" + -0.062*\"毒辣\"\n",
      "0.094*\"蛋糕\" + -0.077*\"麻辣\" + -0.077*\"辣絲絲\" + -0.077*\"辣味\" + -0.077*\"辣乎乎\" + -0.077*\"辛\" + -0.076*\"辛辣\" + -0.060*\"鍋\" + -0.058*\"辣\" + -0.053*\"拉麵\"\n",
      "-0.046*\"拉麵\" + -0.043*\"樂善好施負\" + -0.043*\"臧負\" + -0.043*\"善良負\" + -0.043*\"仁至義盡負\" + -0.043*\"助人為樂負\" + -0.043*\"得以負\" + -0.043*\"何嘗不可負\" + -0.043*\"方可負\" + -0.043*\"足以負\"\n",
      "-0.062*\"鬆餅\" + 0.054*\"毆鬥\" + 0.054*\"毆打\" + 0.054*\"拳打腳踢\" + 0.054*\"揮拳\" + 0.054*\"毆\" + 0.053*\"挹\" + 0.053*\"舀\" + 0.052*\"揪斗\" + 0.052*\"鬥毆\"\n",
      "-0.105*\"拉麵\" + -0.065*\"刈\" + -0.065*\"招徠\" + -0.065*\"攬客\" + -0.065*\"招攬\" + -0.065*\"蛋糕\" + -0.065*\"拉拉\" + -0.065*\"直拉\" + -0.065*\"抻\" + -0.065*\"拉長\"\n",
      "0.062*\"刈\" + 0.062*\"招攬\" + 0.062*\"攬客\" + 0.062*\"招徠\" + 0.062*\"割\" + 0.061*\"直拉\" + 0.061*\"拉拉\" + 0.061*\"抻\" + 0.061*\"拉縴\" + 0.061*\"拉長\"\n",
      "0.106*\"蛋糕\" + 0.080*\"辣味\" + 0.080*\"麻辣\" + 0.080*\"辣乎乎\" + 0.080*\"辛\" + 0.080*\"辣絲絲\" + 0.080*\"辛辣\" + 0.059*\"心狠手辣\" + 0.059*\"狠毒\" + 0.059*\"不顧死活\"\n",
      "0.059*\"別名\" + 0.059*\"別號\" + 0.059*\"別字\" + 0.058*\"哭喊\" + 0.058*\"痛哭流涕\" + 0.058*\"號啕大哭\" + 0.058*\"呼天搶地\" + 0.058*\"如訴如泣\" + 0.058*\"如泣如訴\" + 0.058*\"哭天哭地\"\n"
     ]
    }
   ],
   "source": [
    "print \"\\n\".join(lsi.print_topics(10))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index.save('./zobj/Taipei_shops_ALL_INDEX.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = index = similarities.MatrixSimilarity.load('./zobj/Taipei_shops_ALL_INDEX.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clnSinica(mstr):\n",
    "#     print \">>\", \" \".join(re.findall(p, unicode(mstr.decode(\"utf8\"))))\n",
    "    mstr = mstr.replace('<?xml version=\"1.0\" ?><wordsegmentation version=\"0.1\"><processstatus code=\"0\">Success</processstatus><result>', \"\")\n",
    "    mstr = mstr.replace('</result></wordsegmentation>', \"\")\n",
    "    mstr = mstr.replace('</sentence>', '')\n",
    "    mstr = mstr.replace('<sentence>', '')\n",
    "    return mstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clnTxt(mstr):\n",
    "    return \"\".join([ x.split(\"(\")[0] for x in mstr.strip().split(u\"　\") if len( x.split(\"(\") ) >1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    " \n",
    "def distance_on_unit_sphere(lat1, long1, lat2, long2):\n",
    "    # 以 KM 回傳，原程式碼在 http://goo.gl/JExDdc\n",
    "    # 回傳是公里\n",
    "    # Convert latitude and longitude to \n",
    "    # spherical coordinates in radians.\n",
    "    degrees_to_radians = math.pi/180.0\n",
    "         \n",
    "    # phi = 90 - latitude\n",
    "    phi1 = (90.0 - lat1)*degrees_to_radians\n",
    "    phi2 = (90.0 - lat2)*degrees_to_radians\n",
    "         \n",
    "    # theta = longitude\n",
    "    theta1 = long1*degrees_to_radians\n",
    "    theta2 = long2*degrees_to_radians\n",
    "         \n",
    "    # Compute spherical distance from spherical coordinates.\n",
    "         \n",
    "    # For two locations in spherical coordinates \n",
    "    # (1, theta, phi) and (1, theta, phi)\n",
    "    # cosine( arc length ) = \n",
    "    #    sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n",
    "    # distance = rho * arc length\n",
    "     \n",
    "    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + \n",
    "           math.cos(phi1)*math.cos(phi2))\n",
    "    arc = math.acos( cos )\n",
    " \n",
    "    # Remember to multiply arc by the radius of the earth \n",
    "    # in your favorite set of units to get length.\n",
    "    return arc * 6373\n",
    "\n",
    "def diff_lat_lon(me_loc, target):\n",
    "    return distance_on_unit_sphere(float(me_loc['lat']), float(me_loc['lon']), \n",
    "                                   float(target['lat']), float(target['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_weight = lambda x: 1 if x <= float(1) else 0.9 if x <= float(2) else 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朋友 們 可以 聊天 餐廳 吃 火鍋\n"
     ]
    }
   ],
   "source": [
    "mstr = u\"朋友們可以聊天的餐廳，吃火鍋\"\n",
    "tagged = clnSinica(getPOS(mstr.encode(\"utf8\")))\n",
    "proc_txt = [ x.replace(\"-\", u\"負\") for x in zOpinion(tagged.decode(\"utf8\")).getWords4Tag()]\n",
    "print \" \".join(proc_txt)\n",
    "mstr_bow = dictionary.doc2bow(proc_txt)\n",
    "vec_lsi = lsi[mstr_bow]\n",
    "sims = index[vec_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104473-馬辣頂級麻辣鴛鴦火鍋-信義旗艦店: 0.1987\n",
      "70193-麻辣一村: 0.1941\n",
      "45614-海宴日式和風涮涮屋-台北萬芳旗艦店: 0.1881\n",
      "118931-瘋麻辣頂級鴛鴦麻辣火鍋-西門店: 0.1737\n",
      "34725-天麻蒙古鴛鴦火鍋專賣-公館店: 0.1735\n"
     ]
    }
   ],
   "source": [
    "me_loc = { 'lat': 25.041171, \"lon\":121.565227}\n",
    "\n",
    "shops_val = {}\n",
    "for (x,y) in sorted(enumerate(sims), key=lambda item: -item[1]):\n",
    "    shop_name = txts_pos[x][0]['title']\n",
    "    target = { 'lat': txts_pos[x][0]['latitude'], 'lon': txts_pos[x][0]['longitude'] }\n",
    "    \n",
    "    dist = diff_lat_lon(me_loc, target)\n",
    "    \n",
    "    if not shops_val.has_key(shop_name):\n",
    "        shops_val[shop_name] = []\n",
    "    shops_val[shop_name].append(distance_weight(dist)*y)\n",
    "    \n",
    "\n",
    "shops_rcmd = {}    \n",
    "for shop_name in shops_val.keys():\n",
    "    shops_rcmd[shop_name] = np.average(shops_val[shop_name])\n",
    "    \n",
    "for x in sorted(shops_rcmd.items(), key=operator.itemgetter(1), reverse=True)[:5]:\n",
    "    print \"%s: %.4f\"%(x[0], x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shops_rcmd.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依照店家建立相似度模型後，將特別的字詞與 NTUSD align，最後推出合適的店家。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 地理位置計算 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.041171 121.565227\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import json\n",
    "\n",
    "\n",
    "params = {'address': '台北市政府','sensor': 'false'}\n",
    "url = 'http://maps.googleapis.com/maps/api/geocode/json?' + urllib.urlencode(params)\n",
    "rawreply = urllib2.urlopen(url).read()\n",
    "result = json.loads(rawreply)\n",
    "\n",
    "lat, lng = [(s['formatted_address'],s['geometry']) for s in result['results']][0][1][u'location'].values()\n",
    "print lat, lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0193830566078559"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_on_unit_sphere(23.5852855, 119.61, 23.5852855, 119.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
